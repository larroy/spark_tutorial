{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from spylon_kernel import register_ipython_magics\n",
    "register_ipython_magics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark\n",
    "\n",
    "* Cluster computing software\n",
    "* Extends map-reduce model to queries, dataframes and stream processing abstracting over cluster infrastructure and related complexity\n",
    "* In-memory intermediate storage\n",
    "* Structured data (tables) and semi-structured data (Json, XML) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark components\n",
    "* Language support: Scala, Java, Python, R...\n",
    "* Additional libraries: Spark SQL, ML-Lib, GraphX, Streaming\n",
    "* Base libraries: Spark Core, RDD API, DataFrame API\n",
    "* Cluster Management: Yarn, Mesos, Standalone, K8\n",
    "* Storage / data sources: Local, HDFS, S3, RDBMS, NoSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark cluster architecture\n",
    "\n",
    "Each Spark application gets an independent set of executor JVMs that only run tasks and store data for that application. \n",
    "Key configuration parameters:\n",
    " * spark.executor.cores: max tasks per executor. \n",
    " * spark.executor.memory: max mem per executor.\n",
    "\n",
    "![spark architecture](https://spark.apache.org/docs/latest/img/cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key Spark abstractions & concepts\n",
    "* Job: paralell computation consisting of multiple tasks\n",
    "* RDD (Resilient Distributed Dataset). A collection of elements.\n",
    "    - Immutable\n",
    "    - Partitioned\n",
    "    - Enable efficient data reuse\n",
    "    - fault-tolerant parallel data structures\n",
    "    - Intermediate persistence\n",
    "    - Partition & placement control\n",
    "    - Manipulation through coarse-grained transforms: (map, filter, persist, groupByKey, join...)\n",
    "* Task: Single operation happening on a specific RDD partition\n",
    "* DAG (Directed Acyclic Graph) Scheduler.\n",
    "    - Transforms a logical execution plan of RDD lineage dependencies to a physical execution plan.\n",
    "\n",
    "* DataFrame: 2-dimensional data structure of heterogeneous types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD\n",
    "Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: scala.collection.immutable.Range = Range 0 until 100\n",
       "distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Range(0, 100)\n",
    "val distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doubled: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <console>:26\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val doubled = distData.map(x => x*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: String = org.apache.spark.rdd.MapPartitionsRDD\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubled.getClass.getName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "even: Array[Int] = Array(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198)\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val even = doubled.filter(_ % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Int = 32\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubled.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[RDD documentation https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrame\n",
    "\n",
    "Strongly typed collection of objects that can be transformed in parallel using functional or relational operations.\n",
    "\n",
    "* [DataFrame](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/index.html)\n",
    "\n",
    "\n",
    "* [DataSet](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html)\n",
    "\n",
    "```\n",
    "typeDataFrame = Dataset[Row]\n",
    "val row = Row(1, true, \"a string\", null)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [label: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (1, \"John\", 30, 75),\n",
    "    (0, \"Mary\", 20, 60),\n",
    "    (1, \"Pete\", 51, 80)\n",
    ").toDF(\"label\", \"name\", \"age\", \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+\n",
      "|label|name|age|weight|\n",
      "+-----+----+---+------+\n",
      "|    1|John| 30|    75|\n",
      "|    0|Mary| 20|    60|\n",
      "|    1|Pete| 51|    80|\n",
      "+-----+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightGain: org.apache.spark.sql.Column = (weight + 15)\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weightGain = df(\"weight\") + 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[org.apache.spark.sql.Row] = Array([90], [75], [95])\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(weightGain).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df(\"age\") > 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+\n",
      "|label|name|age|weight|\n",
      "+-----+----+---+------+\n",
      "|    1|John| 30|    75|\n",
      "|    1|Pete| 51|    80|\n",
      "+-----+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"age > 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Long = 2\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(\"age > 20\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+\n",
      "|label|name|age|weight|\n",
      "+-----+----+---+------+\n",
      "|    1|John| 30|    75|\n",
      "+-----+----+---+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "column_exp: org.apache.spark.sql.Column = (age = 30)\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val column_exp = df(\"age\") === 30\n",
    "df.filter(column_exp).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Differences with Pandas \n",
    "\n",
    "Scala:\n",
    "```\n",
    "def filter(condition: Column): Dataset[T]\n",
    "           ^^^^^^^^^^^^^^^^^\n",
    "```\n",
    "\n",
    "Most functions in Spark accept column expressions\n",
    "\n",
    "Pandas:\n",
    "\n",
    "Pandas is eager execution, and indexing is done with boolean series\n",
    "\n",
    "```\n",
    "In [27]: import pandas as pd \n",
    "    ...: df = pd.DataFrame([ \n",
    "    ...:     [1, 'John', 30, 75], \n",
    "    ...:     [1, 'Mary', 20, 60], \n",
    "    ...:     [1, 'Pete', 51, 80], \n",
    "    ...: ]) \n",
    "    ...: df.columns = ['label','name','age','weight'] \n",
    "    ...:  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "In [28]: df                                                                                                             \n",
    "Out[28]: \n",
    "   label  name  age  weight\n",
    "0      1  John   30      75\n",
    "1      1  Mary   20      60\n",
    "2      1  Pete   51      80\n",
    "In [29]: df['age']>20                                                                                                   \n",
    "Out[29]: \n",
    "0     True\n",
    "1    False\n",
    "2     True\n",
    "Name: age, dtype: bool\n",
    "In [30]: df[df['age']>20]                                                                                               \n",
    "Out[30]: \n",
    "   label  name  age  weight\n",
    "0      1  John   30      75\n",
    "2      1  Pete   51      80\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
